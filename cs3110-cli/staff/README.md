cs3110-cli
==========

Command line tools for cs3110, staff edition.
The solution for compiling, testing, and running your code.

For instructions on installing the tool and using the base commands, see the student README.
This document just explains the mass testing commands.

Commands are:

* `cs3110 clean` remove the files generated through compilation.
* `cs3110 compile <filename>` compile the target file, generating corresponding `.cmo` and `.cmi` files in the `_build` directory.
* `cs3110 diff <dir_names>` compares the files stored in the directory `_nocompile` with the files in the supplied `dir_names`.
* `cs3110 doc [src_dir] <output_dir>` generates [ocamldoc](http://caml.inria.fr/pub/docs/manual-ocaml-400/manual029.html) documentation from the `.mli` files in `src_dir` and dumps the generated html into `output_dir`. The default `src_dir` is the current working directory.
* `cs3110 email` Send all the emails generated by running the smoke test.
* `cs3110 harness <dir_names>` Runs the tests stored in `./tests` within each directory of the supplied `dir_names`.
* `cs3110 help` print a quick summary of the available commands.
* `cs3110 reverse <test_name> <dir_names>` Runs a specific test (indicated by `test_name`, implemented within each of `dir_names`) against dummy solutions stored in `reverse_dir`.
* `cs3110 rubric <solution_dir>` Create a rubric for the suite of tests in `./tests`. Uses the argument folder to compile the tests.
* `cs3110 run <filename> [args]` Run the executable created by `cs3110 compile <filename>` with the supplied arguments.
* `cs3110 smoke <dir_names>` Compile many files within each dir of `dir_names`. Files to compile are determined by the file `./smoke_test`.
* `cs3110 test <filename>` Execute the `pa_ounit` tests within the file.

Detailed descriptions follow.
These are a superset of the commands in the student version.
New commands automate mass testing.

NEW Command Summary
===================

The staff tool lets us run a smoke test, send emails to students who failed to compile, diff previous no-compiles with updated submissions, run a full test harness generating a CMS-ready spreadsheet, and run a reversed test harness that checks the completeness of students' test cases.
We organize command descriptions by the goals they accomplish, and break those sections into the broad goals, implementation specifics, and usage instructions.

Smoke Testing
-------------

##### Overview #####

Smoke testing is pre-testing.
The goal is to ensure that submissions entered on Thursday will compile when we grade on Sunday.
The smoke test uses `cs3110 compile` for each file to make sure that students' code compiles.

Additionally, the smoke test saves information about which files failed to compile.
This lets us send emails to students who had compiler errors/warnings and compare later submissions to the initial, Thursday night deadline.

##### Implementation #####

Magic files:

* `./_email` : directory containing email messages. Contains text files of form "<netid>.txt".
* `./_nocompile` : directory containing files that failed to compile. Contains a bunch of folders containing `.ml` files. For example, "./_nocompile/<netid>/<filename>.ml".
* `./smoke_test` : file containing a list of filenames. Sample contents are "part1\npart2\npart3\n". This would require the harness to test students' `part1.ml`, `part2.ml`, and `part3.ml`.

The email directory is only important until students who failed to compile are notified.
The no-compile directory is very important, and must be saved.
It is a reference to compare all students' future submissions against their initial, Thursday 11:59pm submission.
Yes, you can re-download the original submission on CMS, but that's not a happy large-scale exercise.

`cs3110 smoke` runs as follows:

* Input is a list of directories. Each should be named with a valid Cornell netid. Example: `cs3110 smoke Submissions/blg59`.
* Parses the input list of directories, resolving @-notation, if present, to create a list of directories.
* Changes directory into each inputted directory and compiles each target file. Targets are named in the `smoke_test` file.
* If a file does not compile, a copy of it is saved to the no-compile directory in a folder named after the input directory (i.e. the netid)
* An email message is generated for each netid that had at least one no-compile. The email lists all files which did not build.

##### Usage #####

Assuming you have downloaded the submissions from CMS (see the FAQ), you should have a folder called `Submissions` containing a directory for each student.
First, decide which files need to compile and create the `smoke_test` file.
`echo "folding\npart2" > smoke_test`, or whatever.
Second, run the smoke test.
`cs3110 smoke Submission/blg59` tests a single student, `blg59`, and `cs3110 smoke Submission/*` tests every student.
The email and no-compile directories will be generated automatically.

Before re-running the smoke test, remember to delete or move the email and no-compile directories.
They could mess things up otherwise.

Email
-----

##### Overview #####

`cs3110 email` sends email messages to students.
Use this power only for good.

##### Implementation #####

Magic files:

* `./_email`: a directory containing email messages. For example, `_email/blg59.txt`

The email script will:

* Read the contents of the magic email directory
* Infer a recipient from the filename. `_email/blg59.txt` will resolve to recipient `blg59@cornell.edu`.
* Use the contents of the text file as the body of the message. Add a default subject.
* Send the email message using the Unix `mail` utility.

The email script _will not_ let you specify the sender field.
I am unsure how to do this.
For now, the sender is your username at your hostname.
My emails are send as `Î»<ben@rrsdhcp-10-32-33-136.redrover.cornell.edu>`.

Which, is not all that bad.
It preserves accountability.

##### Usage #####

Call `cs3110 email`. No arguments!

Diff
----

##### Overview #####

The goal of diff is to give students grace from no-compiles.
If they have a tiny error that causes their code to compile, they are allowed a tiny resubmission without penalty.
But not a big resubmission!
Unfortunately the only way to differentiate "tiny" vs. "big" is by hand.

`cs3110 diff` compares old and new submissions.
It takes output saved from a previous submission and compares it to new source files.
Standard operation should be that we run the smoke test on Thursday, generate the no-compile directory, re-download submission on Saturday night and run diff.

##### Implementation #####

Magic files:

* `./_nocompile`: this is the directory generated by the smoke test.
* `./_diff_results`: a .csv file containing judgements for each resubmission for each file that had not previously compiled.

Judgements are integers, and either "0", or "1".
"1" is good, it's OK!
"0" is bad, it's not ok.

`cs3110 diff Submissions/blg59` behaves as follows:

* Check whether the folder `_nocompile/blg59` exists. If not, do nothing. The student had previously compiled, so CMS's judgement on whether the student is late is accurate.
* If no submission exists, perform a diff on each of the students' files.
* Missing files and null diffs are automatically accepted.
* User is prompted with non-null diffs and asked for a "0" (bad) or "1" (ok)
* Results are saved in the diff results csv file. These must be parsed manually later, when we add late penalties on CMS.

This procedure is iterated over each argument passed to `diff`.

##### Usage #####

`diff` takes a list of directory names.
`cs3110 diff Submissions/*` is most likely the desired behavior; it works just the same as the arguments to the smoke test.

Rubric
------

##### Overview #####

The harness depends on a rubric.
It seeks to create a spreadsheet ready for upload to CMS, thus it needs a point value for each function.
`cs3110 rubric <solutions>` helps create a rubric file that the harness can read.

##### Implementation #####

Magic files:

* `./rubric.yaml` : the rubric itself
* `./tests` : the folder of unit tests, also used by the harness.

You may create your own rubric at any time.
Each line should contain the name of a test, a colon, and a point value for the test.
Observe:

    first_test_ever : 42

Now the student will earn 42 points upon passing the test.
Separate groups of tests with a hash.
Something like:

    # part1_test
    test1 : 5
    yoloswag : 15
    # part2_test
    heres_another_test : 5

These tests will be grouped into different columns of the spreadsheet.
One column will be worth 20 points total and the other column 5 points total.

`cs3110 rubric <solutions>` compiles each test in the magic tests directory and extracts the test names out of them.
The argument `<solutions>` is an implementation of the source files.
It provides the dependencies each test needs to compile.
(If there exists a way to get test names from the files without compiling to bytecode, we should convert to that system.)
The user is prompted for a point value for each function and these results are saved in the rubric.

##### Usage #####

Another simple command to use.
`cs3110 rubric release` is sufficient to compile everything and generate a rubric, provided `release` is a folder containing stub implementations.
The tests are never run, so it doesn't matter that the release files fail them.

Harness
-------

##### Overview #####

This is the CS 3110 test harness.
It takes a groups of students and runs a group of tests on each one, recording the results in a few ways.

##### Implementation #####

Magic files:

* `./tests` : A directory containing a bunch of `.ml` files containing test cases.
* `./rubric.yaml` : The rubric for the harness. Assigns a point value for each unit test of each file of the tests directory.
* `./_output` : A directory where student printouts are stored.

The harness runs as follows for each student directory in the argument:

* Check that a rubric exists. If not, call `cs3110 rubric` on a compiling student submission.
* Change directories into the student's implementation
* Copy in the test files, compile and run each in sequence.
* Save outputs, change back to the root directory.

The harness generates lots of output.
Really, most of the implementation is output generation.

First, there is the console.
The netid of each student is printed as they are tested, along with compiler output and the results for each test.
These prints are roughly in markdown format, for readability.

Next, we have markdown files summarizing all tests for each student.
These are close to what appears on screen, just easier to retrieve.
Additionally, they are printed on CMS as grader comments.

Next, there is the postscript.
A .ps file of each student's source code and test results is generated and saved in the output directory.
The source code for the implementation used by `file_test.ml` is found by stripping the trailing `_test` and searching for `file.ml`.
This means that the `_test` convention is required, at least if you want source code in the postscript documents.
Test output, printed below the source code, is about the same as what is printed on the terminal.
The main difference with postscript is that we have one document for each file, in case we want to go back to on-paper grading.

Lastly, there is a spreadsheet of scores.
This has columns for netids, point totals corresponding to each test file, and grader comments (i.e. the test harness results).
It does NOT have a total column.
Reason being that if the total is not input, CMS will calculate totals on its own every time scores are updated.
Otherwise, it uses the uploaded value of the total.

##### Usage #####

Call `cs3110 harness <targets>` with the student submissions directories.
The arguments are parsed just like in the smoke test--you may use an @-symbol and all that.

Reverse
-------

##### Overview #####

The reverse test harness grades a student based on how many errors their tests catch.
It requires a few dummy implementations and a corresponding rubric.
The goal is to test the quality of student-written unit tests.

##### Implementation #####

Magic files:

* `./reverse_rubric.yaml` : The rubric assigning points to implementations, based on whether they pass or fail according to expectation.
* `./_output` : The directory for printed results. Namely, for the spreadsheet of total scores.
* `./reverse_tests` : Directory of solution implementations. Some should pass and some should fail.

The reverse harness creates a rubric, if one does not exist, of format `<impl_dir> : <Y/N> : <point_value>`.
Observe:

    passing_implementation : Y : 10
    failing_implementation : N : 3

This says that there is a directory `passing_implementation` that ought to pass all tests and is worth 10 points.
Additionally, there is a directory `failing_implementation` that should cause some tests to fail and is worth 3 points.

For each fake implementation, the student's test case is copied into the directory and executed.
The reverse harness records whether any test failed and matches that with the expectation.
If the expectation and result match, the student is awarded points.
The point totals are accumulated and saved into a CMS-ready spreadsheet.


DISCLAIMER: The reverse harness is poorly coded.
It shares some functionality with the regular harness, but this code is duplicated, not abstracted.
If the reverse harness winds up being useful, this code will be rewritten.

##### Usage #####

`cs3110 reverse <test_name> <targets>` searches each directory in `<targets>` for a file called `<test_name>` and runs it against each dummy implementation.

Writing Tests
=============

There are a few rules about writing tests for the harness.

* Test names may not contain whitespace. For now.
* The test for `file.ml` should be called `file_test.ml`.

These rules help the harness do a little magic and make things (hopefully) clearer for staff and students.
Please complain if you don't like these restrictions.

FAQ
===

Q. How do I set a timeout on a test?
------------------------------------

Wrap the call in the `Assertions.timeout` wrapper, from the `Assertions` library, which is linked to every executable generated by `cs3110 compile`.
This function is documented in `assertions.mli`.

Q. How do I download files from CMS?
------------------------------------

1. Navigate to the course page on CMS
2. In the sidebar, select the "groups" page for the assignment you want files for.
3. Click "All", the red word in the top-center of the page. This selects all students in the table that fills the rest of the page.
4. Click "Files", the grey button at the top-right of the page.
5. Select a download location and unzip the result. Party!

Credits
=======

We thank [Arjun Guha](https://github.com/arjunguha/cs691f) at Brown University, [Jane Street](https://github.com/janestreet/pa_ounit), and [OUnit](https://github.com/warrenharris/ounit).
